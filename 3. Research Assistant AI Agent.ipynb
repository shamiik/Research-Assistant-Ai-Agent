{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPSFJ3wz7cJ+0lSAmbTq7y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkgD0__Q45DE","executionInfo":{"status":"ok","timestamp":1760733916660,"user_tz":-360,"elapsed":46852,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}},"outputId":"7e3d20fd-c00f-420e-9807-2e0ced6941d7","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n","Collecting langgraph\n","  Downloading langgraph-1.0.0-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.35)\n","Collecting langchain-groq\n","  Downloading langchain_groq-1.0.0-py3-none-any.whl.metadata (1.7 kB)\n","Collecting langchain_community\n","  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n","Collecting pypdf\n","  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting chromadb\n","  Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n","Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n","  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n","Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph)\n","  Downloading langgraph_prebuilt-1.0.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n","  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.3)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n","Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n","  Downloading groq-0.32.0-py3-none-any.whl.metadata (16 kB)\n","INFO: pip is looking at multiple versions of langchain-groq to determine which version is compatible with other requirements. This could take a while.\n","Collecting langchain-groq\n","  Downloading langchain_groq-0.3.8-py3-none-any.whl.metadata (2.6 kB)\n","INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n","Collecting langchain_community\n","  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n","Collecting requests<3,>=2 (from langchain)\n","  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.0)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n","Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n","Collecting pybase64>=1.4.1 (from chromadb)\n","  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n","Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n","  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n","Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n","  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n","  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n","Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n","  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n","Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n","Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n","Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Downloading langgraph-1.0.0-py3-none-any.whl (155 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_groq-0.3.8-py3-none-any.whl (16 kB)\n","Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading groq-0.32.0-py3-none-any.whl (135 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_prebuilt-1.0.0-py3-none-any.whl (28 kB)\n","Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n","Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Building wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=2aae0e07b46aec4ced3e2bce594527c5db8f6dddd793057d4fac1052ac889487\n","  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n","Successfully built pypika\n","Installing collected packages: pypika, durationpy, uvloop, urllib3, pypdf, pybase64, ormsgpack, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httptools, bcrypt, backoff, watchfiles, typing-inspect, requests, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, langgraph-sdk, groq, dataclasses-json, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, langchain-groq, chromadb, langgraph-prebuilt, langgraph, langchain_community\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.5.0\n","    Uninstalling urllib3-2.5.0:\n","      Successfully uninstalled urllib3-2.5.0\n","  Attempting uninstall: opentelemetry-proto\n","    Found existing installation: opentelemetry-proto 1.37.0\n","    Uninstalling opentelemetry-proto-1.37.0:\n","      Successfully uninstalled opentelemetry-proto-1.37.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.4\n","    Uninstalling requests-2.32.4:\n","      Successfully uninstalled requests-2.32.4\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n","    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.37.0\n","    Uninstalling opentelemetry-api-1.37.0:\n","      Successfully uninstalled opentelemetry-api-1.37.0\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n","    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.37.0\n","    Uninstalling opentelemetry-sdk-1.37.0:\n","      Successfully uninstalled opentelemetry-sdk-1.37.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n","google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.1.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 groq-0.32.0 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 langchain-groq-0.3.8 langchain_community-0.3.31 langgraph-1.0.0 langgraph-checkpoint-2.1.2 langgraph-prebuilt-1.0.0 langgraph-sdk-0.2.9 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 onnxruntime-1.23.1 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 ormsgpack-1.11.0 posthog-5.4.0 pybase64-1.4.2 pypdf-6.1.1 pypika-0.48.9 requests-2.32.5 typing-inspect-0.9.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"]}],"source":["!pip install langchain langgraph langsmith langchain-groq langchain_community pypdf chromadb"]},{"cell_type":"code","source":["import os\n","import uuid\n","from typing import TypedDict, Annotated, List, Dict, Any, Optional\n","\n","# LangGraph / LangChain\n","from langgraph.graph import StateGraph, START, END\n","from langgraph.prebuilt import ToolNode, tools_condition\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph.message import add_messages\n","from langchain_core.tools import tool\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langchain_groq import ChatGroq\n","\n","from pypdf import PdfReader\n","\n","# Vector DB (Chroma)\n","import chromadb\n","from chromadb.utils import embedding_functions\n","from chromadb.config import Settings"],"metadata":{"id":"Z8S8cd7Ni7R4","executionInfo":{"status":"ok","timestamp":1760733945225,"user_tz":-360,"elapsed":23800,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Model\n","\n","groq_api_key = \"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\"\n","LLM_MODEL_NAME = \"openai/gpt-oss-20b\"\n","\n","llm = ChatGroq(model=LLM_MODEL_NAME, groq_api_key=groq_api_key)\n","\n","\n","# Vector DB Setup\n","chroma_client = chromadb.PersistentClient(\n","    path=\"./chroma_db\",\n","    settings=Settings(anonymized_telemetry=False)\n",")\n","embedding_function = embedding_functions.DefaultEmbeddingFunction()\n","\n","# Long-term memory collection (persistent across sessions)\n","LT_COLLECTION_NAME = \"ra_longterm_insights\"\n","try:\n","    lt_collection = chroma_client.get_collection(\n","        name=LT_COLLECTION_NAME,\n","        embedding_function=embedding_function\n","    )\n","except Exception:\n","    lt_collection = chroma_client.create_collection(\n","        name=LT_COLLECTION_NAME,\n","        embedding_function=embedding_function\n","    )\n"],"metadata":{"id":"nWQ98gjgjDYk","executionInfo":{"status":"ok","timestamp":1760735094122,"user_tz":-360,"elapsed":285,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Utilities\n","def extract_text_from_pdf(pdf_path: str) -> str:\n","    \"\"\"Extract text from a PDF.\"\"\"\n","    reader = PdfReader(pdf_path)\n","    texts = []\n","    for page in reader.pages:\n","        try:\n","            texts.append(page.extract_text() or \"\")\n","        except Exception:\n","            texts.append(\"\")\n","    return \"\\n\".join(texts).strip()\n","\n","def simple_chunk_text(text: str, chunk_size: int = 1200, chunk_overlap: int = 200) -> List[str]:\n","    \"\"\"Paragraph-first chunker with light sentence fallback and overlap.\"\"\"\n","    if not text:\n","        return []\n","    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n","    chunks, buf, cur_len = [], [], 0\n","\n","    def _flush_with_overlap():\n","        nonlocal buf, cur_len\n","        if buf:\n","            chunks.append(\" \".join(buf).strip())\n","            if chunk_overlap > 0 and chunks[-1]:\n","                overlap_tokens = chunks[-1].split()[-(chunk_overlap // 6):]\n","                buf = [\" \".join(overlap_tokens)]\n","                cur_len = len(buf[0])\n","            else:\n","                buf, cur_len = [], 0\n","\n","    for p in paras:\n","        if len(p) > chunk_size:\n","            sentences = p.replace(\"\\n\", \" \").split(\". \")\n","            for s in sentences:\n","                s2 = (s + (\"\" if s.endswith(\".\") else \".\")) if s else \"\"\n","                if cur_len + len(s2) + 1 > chunk_size and buf:\n","                    _flush_with_overlap()\n","                buf.append(s2)\n","                cur_len += len(s2) + 1\n","        else:\n","            if cur_len + len(p) + 2 > chunk_size and buf:\n","                _flush_with_overlap()\n","            buf.append(p)\n","            cur_len += len(p) + 2\n","\n","    if buf:\n","        chunks.append(\" \".join(buf).strip())\n","\n","    return [c for c in chunks if c and len(c) > 10]\n","\n","def get_st_collection_name(session_id: str) -> str:\n","    return f\"st_pdf_{session_id}\"\n","\n","def get_or_create_st_collection(session_id: str):\n","    \"\"\"Short-term per-session Chroma collection for current PDFs.\"\"\"\n","    name = get_st_collection_name(session_id)\n","    try:\n","        return chroma_client.get_collection(\n","            name=name,\n","            embedding_function=embedding_function\n","        )\n","    except Exception:\n","        return chroma_client.create_collection(\n","            name=name,\n","            embedding_function=embedding_function\n","        )\n","\n","def upsert_chunks_to_collection(collection, doc_id: str, chunks: List[str], extra_meta: Optional[Dict[str, Any]] = None):\n","    \"\"\"Add chunks to a Chroma collection with metadata.\"\"\"\n","    if not chunks:\n","        return\n","    docs, ids, metadatas = [], [], []\n","    for i, ch in enumerate(chunks):\n","        docs.append(ch)\n","        ids.append(f\"{doc_id}_{i}\")\n","        md = {\"doc_id\": doc_id, \"chunk_index\": i}\n","        if extra_meta:\n","            md.update(extra_meta)\n","        metadatas.append(md)\n","    collection.add(documents=docs, ids=ids, metadatas=metadatas)\n","\n","def query_collection(collection, query: str, n: int = 5, where: Optional[Dict[str, Any]] = None) -> List[str]:\n","    \"\"\"Return top documents text from Chroma query.\"\"\"\n","    try:\n","        res = collection.query(query_texts=[query], n_results=n, where=where or {})\n","        return res.get(\"documents\", [[]])[0]\n","    except Exception:\n","        return []\n"],"metadata":{"id":"vWrwyZiXIO06","executionInfo":{"status":"ok","timestamp":1760735094853,"user_tz":-360,"elapsed":34,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Tools\n","@tool\n","def ingest_pdfs(paths: List[str], session_id: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Ingest PDFs for the current session: extract → chunk → embed to short-term store.\n","    Returns stats per file.\n","    \"\"\"\n","    st_collection = get_or_create_st_collection(session_id)\n","    stats = []\n","    for p in paths:\n","        text = extract_text_from_pdf(p)\n","        chunks = simple_chunk_text(text)\n","        doc_id = f\"{session_id}::{os.path.basename(p)}::{uuid.uuid4().hex[:8]}\"\n","        upsert_chunks_to_collection(\n","            st_collection,\n","            doc_id=doc_id,\n","            chunks=chunks,\n","            extra_meta={\"session_id\": session_id, \"file\": os.path.basename(p)}\n","        )\n","        stats.append({\"file\": os.path.basename(p), \"chunks\": len(chunks), \"doc_id\": doc_id})\n","    return {\"indexed_files\": stats}\n","\n","@tool\n","def summarize_current_pdf(query: str, session_id: str, k: int = 6) -> str:\n","    \"\"\"\n","    Summarize/answer from SHORT-TERM (current session PDFs).\n","    Returns a concise answer with [S#] citations.\n","    \"\"\"\n","    st_collection = get_or_create_st_collection(session_id)\n","    docs = query_collection(st_collection, query, n=k, where={\"session_id\": session_id})\n","    if not docs:\n","        return \"No session PDF content found.\"\n","    system = SystemMessage(content=\"You are a research assistant. Summarize accurately and cite inline with [S#] indices for provided snippets.\")\n","    context_labelled = \"\\n\\n\".join([f\"[S{i+1}] {d}\" for i, d in enumerate(docs)])\n","    user = HumanMessage(content=f\"Use the snippets below to answer: {query}\\n\\nSnippets:\\n{context_labelled}\")\n","    out = ChatGroq(model=LLM_MODEL_NAME, groq_api_key=groq_api_key).invoke([system, user])\n","    return out.content\n","\n","@tool\n","def keyword_extract(text: str, top_k: int = 10) -> List[str]:\n","    \"\"\"\n","    Extract up to top_k key terms/phrases from a passage.\n","    \"\"\"\n","    prompt = f\"Extract up to {top_k} key terms or short phrases from the text below. Return as a bullet list.\\n\\nTEXT:\\n{text}\"\n","    out = ChatGroq(model=LLM_MODEL_NAME, groq_api_key=groq_api_key).invoke([HumanMessage(content=prompt)])\n","    lines = [ln.strip(\"-• \").strip() for ln in out.content.splitlines() if ln.strip()]\n","    uniq = []\n","    for l in lines:\n","        if l and l not in uniq:\n","            uniq.append(l)\n","    return uniq[:top_k]\n","\n","\n","@tool\n","def save_insight(note: str, tags: Optional[List[str]] = None, source: Optional[str] = None) -> str:\n","    \"\"\"\n","    Save an insight/summary/FAQ to LONG-TERM memory (persistent).\n","    \"\"\"\n","    insight_id = f\"ins_{uuid.uuid4().hex[:10]}\"\n","    tags_str = \", \".join(tags) if tags else \"\"\n","    meta = {\"type\": \"insight\", \"tags\": tags_str, \"source\": source or \"\"}\n","    lt_collection.add(documents=[note], ids=[insight_id], metadatas=[meta])\n","    return f\"Saved long-term insight: {insight_id}\"\n","\n","\n","\n","@tool\n","def ask_with_memory(query: str, session_id: str, k_st: int = 5, k_lt: int = 5) -> str:\n","    \"\"\"\n","    Answer a question by blending SHORT-TERM (current PDFs) + LONG-TERM (insights) contexts.\n","    Prefers ST evidence when conflicting. Cites with [ST#]/[LT#].\n","    \"\"\"\n","    st_collection = get_or_create_st_collection(session_id)\n","    st_docs = query_collection(st_collection, query, n=k_st, where={\"session_id\": session_id}) or []\n","    lt_docs = query_collection(lt_collection, query, n=k_lt) or []\n","\n","    if not (st_docs or lt_docs):\n","        return \"I have no relevant context in short-term or long-term memory.\"\n","\n","    labelled = []\n","    idx = 1\n","    for d in st_docs:\n","        labelled.append((f\"[ST{idx}]\", d))\n","        idx += 1\n","    for d in lt_docs:\n","        labelled.append((f\"[LT{idx}]\", d))\n","        idx += 1\n","\n","    context_blob = \"\\n\\n\".join([f\"{tag} {text}\" for tag, text in labelled])\n","    prompt = [\n","        SystemMessage(content=\"You are a precise research assistant. Use ONLY the provided context unless the question is general knowledge.\"),\n","        HumanMessage(content=f\"Question: {query}\\n\\nContext:\\n{context_blob}\\n\\nInstructions:\\n\"\n","                             f\"- Prefer short-term (ST#) evidence when it conflicts with long-term (LT#).\\n\"\n","                             f\"- Cite sources inline with their tag numbers.\\n\"\n","                             f\"- If insufficient context, say so and ask for a PDF or clarification.\")\n","    ]\n","    out = ChatGroq(model=LLM_MODEL_NAME, groq_api_key=groq_api_key).invoke(prompt)\n","    return out.content\n","\n","@tool\n","def multiply(a: int, b: int) -> int:\n","    \"\"\"This function is responsible for multiplication\"\"\"\n","    return a * b\n"],"metadata":{"id":"kxSBTUI5IUZ6","executionInfo":{"status":"ok","timestamp":1760735096928,"user_tz":-360,"elapsed":67,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# BONUS 1. PDF Summaries - Long-Term\n","\n","def _gather_session_docs(session_id: str):\n","    \"\"\"Return (st_collection, files -> {doc_ids, count}, total_chunks).\"\"\"\n","    st_collection = get_or_create_st_collection(session_id)\n","    try:\n","        payload = st_collection.get(where={\"session_id\": session_id}, include=[\"metadatas\", \"documents\", \"ids\"])\n","    except Exception:\n","        return st_collection, {}, 0\n","\n","    files_map: Dict[str, Dict[str, Any]] = {}\n","    total_chunks = 0\n","    for md, _doc, _id in zip(payload.get(\"metadatas\", []), payload.get(\"documents\", []), payload.get(\"ids\", [])):\n","        if not md:\n","            continue\n","        f = md.get(\"file\", \"unknown\")\n","        files_map.setdefault(f, {\"doc_ids\": set(), \"count\": 0})\n","        files_map[f][\"doc_ids\"].add(md.get(\"doc_id\", \"\"))\n","        files_map[f][\"count\"] += 1\n","        total_chunks += 1\n","\n","    for f in files_map:\n","        files_map[f][\"doc_ids\"] = list(files_map[f][\"doc_ids\"])\n","    return st_collection, files_map, total_chunks\n","\n","def _summarize_text_blocks(blocks: List[str], title: str) -> str:\n","    \"\"\"Map-reduce style summary for many chunks.\"\"\"\n","    map_msgs = [\n","        SystemMessage(content=\"You are a careful scientific summarizer. Extract key points faithfully.\"),\n","        HumanMessage(content=f\"Summarize the following {len(blocks)} passages into bullet points. Be concise, no speculation.\\n\\n\" +\n","                             \"\\n\\n\".join([f\"[{i+1}] {b}\" for i, b in enumerate(blocks[:12])]))\n","    ]\n","    mapped = ChatGroq(model=LLM_MODEL_NAME, groq_api_key=groq_api_key).invoke(map_msgs).content\n","\n","    reduce_msgs = [\n","        SystemMessage(content=\"You write tight research summaries.\"),\n","        HumanMessage(content=f\"Create a 3–5 sentence abstract and 5 bullets from these notes about '{title}':\\n\\n{mapped}\")]\n","    return ChatGroq(model=LLM_MODEL_NAME, groq_api_key=groq_api_key).invoke(reduce_msgs).content\n","\n","@tool\n","def summarize_pdfs_to_longterm(session_id: str, files: Optional[List[str]] = None, k_per_file: int = 40) -> str:\n","    \"\"\"\n","    Create and store long-term summaries for PDFs ingested in this session.\n","    - If `files` is None, summarize all files in the session.\n","    - Stores each summary in LT memory with tags=['pdf-summary', <file>].\n","    \"\"\"\n","    st_collection, files_map, _ = _gather_session_docs(session_id)\n","    if not files_map:\n","        return \"No session PDFs found to summarize.\"\n","\n","    target_files = files or list(files_map.keys())\n","    saved = []\n","\n","    for file_name in target_files:\n","\n","        docs = query_collection(\n","            st_collection,\n","            f\"Main ideas and contributions of {file_name}\",\n","            n=k_per_file,\n","            where={\"session_id\": session_id, \"file\": file_name}\n","        )\n","        if not docs:\n","            payload = st_collection.get(where={\"session_id\": session_id, \"file\": file_name}, include=[\"documents\"])\n","            docs = (payload.get(\"documents\") or [])[:k_per_file]\n","\n","        if not docs:\n","            continue\n","\n","        summary = _summarize_text_blocks(docs, title=file_name)\n","        insight_id = f\"ins_{uuid.uuid4().hex[:10]}\"\n","\n","        lt_collection.add(\n","            documents=[summary],\n","            ids=[insight_id],\n","            metadatas=[{\"type\": \"pdf_summary\", \"tags\": f\"pdf-summary,{file_name}\", \"source\": file_name}]\n","        )\n","        saved.append({\"file\": file_name, \"insight_id\": insight_id})\n","\n","    if not saved:\n","        return \"No summaries created.\"\n","    return \"Saved PDF summaries to long-term: \" + \", \".join([f\"{s['file']}→{s['insight_id']}\" for s in saved])\n","\n","\n","# BONUS 2. Multi-Document Cross-Referencing\n","\n","def _per_file_topk(st_collection, session_id: str, query: str, k_per_file: int = 3) -> Dict[str, List[str]]:\n","    \"\"\"Return {file: [chunks]} with top-k matches per file.\"\"\"\n","    _, files_map, _ = _gather_session_docs(session_id)\n","    result: Dict[str, List[str]] = {}\n","    for file_name in files_map.keys():\n","        docs = query_collection(\n","            st_collection, query, n=k_per_file,\n","            where={\"session_id\": session_id, \"file\": file_name}\n","        )\n","        if docs:\n","            result[file_name] = docs\n","    return result\n","\n","@tool\n","def cross_reference(query: str, session_id: str, k_per_file: int = 3, k_lt: int = 5) -> str:\n","    \"\"\"\n","    Compare evidence across multiple PDFs (ST) and augment with LT insights.\n","    Cites as [DOC:<file>#i] and [LT#j]. Prefer DOC when conflicting.\n","    \"\"\"\n","    st_collection = get_or_create_st_collection(session_id)\n","    per_file = _per_file_topk(st_collection, session_id, query, k_per_file=k_per_file)\n","    lt_docs = query_collection(lt_collection, query, n=k_lt) or []\n","\n","    if not per_file and not lt_docs:\n","        return \"No relevant evidence found in session PDFs or long-term memory.\"\n","\n","    tagged_blocks = []\n","    for f, blocks in per_file.items():\n","        for i, b in enumerate(blocks, start=1):\n","            tagged_blocks.append((f\"[DOC:{f}#{i}]\", b))\n","    for j, b in enumerate(lt_docs, start=1):\n","        tagged_blocks.append((f\"[LT{j}]\", b))\n","\n","    context = \"\\n\\n\".join([f\"{tag} {txt}\" for tag, txt in tagged_blocks])\n","    prompt = [\n","        SystemMessage(content=\"Compare and contrast evidence across documents. Be precise and cite tags.\"),\n","        HumanMessage(content=f\"Query: {query}\\n\\nEvidence:\\n{context}\\n\\n\"\n","                             f\"Instructions:\\n- Note agreements/disagreements across PDFs.\\n\"\n","                             f\"- Prefer PDF (DOC) evidence over LT when conflicting.\\n\"\n","                             f\"- Use clear, short paragraphs and inline citations.\")]\n","    out = ChatGroq(model=LLM_MODEL_NAME, groq_api_key=groq_api_key).invoke(prompt)\n","    return out.content\n","\n","\n","# BONUS 3. Memory & Retrieval Visualization\n","\n","def _count_lt() -> int:\n","    try:\n","        payload = lt_collection.get(include=[])\n","        return len(payload.get(\"ids\", []))\n","    except Exception:\n","        return 0\n","\n","@tool\n","def memory_dashboard(session_id: str, last_query: Optional[str] = None) -> Dict[str, Any]:\n","    \"\"\"\n","    Return a JSON dashboard of memory usage and (optional) retrieval trace.\n","    \"\"\"\n","    st_collection, files_map, total_chunks = _gather_session_docs(session_id)\n","    session_files = [{\"file\": f, \"chunks\": info[\"count\"]} for f, info in files_map.items()]\n","    lt_count = _count_lt()\n","\n","    dash: Dict[str, Any] = {\n","        \"session\": {\"files\": sorted(session_files, key=lambda x: x[\"file\"]), \"total_chunks\": total_chunks},\n","        \"long_term\": {\"total_items\": lt_count}\n","    }\n","\n","    if last_query:\n","        per_file = _per_file_topk(st_collection, session_id, last_query, k_per_file=5)\n","        lt_docs = query_collection(lt_collection, last_query, n=5) or []\n","        dash[\"retrieval\"] = {\n","            \"query\": last_query,\n","            \"per_file_hits\": {f: len(v) for f, v in per_file.items()},\n","            \"lt_hits\": len(lt_docs)\n","        }\n","\n","    return dash\n","\n","@tool\n","def memory_dashboard_markdown(session_id: str, last_query: Optional[str] = None) -> str:\n","    \"\"\"\n","    Pretty Markdown view of the memory dashboard for quick visualization in-chat.\n","    \"\"\"\n","    data = memory_dashboard(session_id, last_query)\n","    ses = data[\"session\"]; lt = data[\"long_term\"]\n","    lines = [\"# Memory Dashboard\",\n","             f\"**Session:** `{session_id}`\",\n","             \"## Short-Term (Session PDFs)\",\n","             \"\",\n","             \"| File | Chunks |\",\n","             \"|---|---|\"]\n","    for item in ses[\"files\"]:\n","        lines.append(f\"| {item['file']} | {item['chunks']} |\")\n","    lines += [\"\", f\"**Total session chunks:** {ses['total_chunks']}\", \"\",\n","              \"## Long-Term Memory\",\n","              f\"**Total items:** {lt['total_items']}\"]\n","\n","    if \"retrieval\" in data:\n","        r = data[\"retrieval\"]\n","        lines += [\"\", \"## Retrieval Trace\",\n","                  f\"**Query:** {r['query']}\",\n","                  \"\", \"| Source | Hits |\", \"|---|---|\"]\n","        for f, k in r[\"per_file_hits\"].items():\n","            lines.append(f\"| {f} | {k} |\")\n","        lines.append(f\"| Long-Term | {r['lt_hits']} |\")\n","\n","    return \"\\n\".join(lines)\n","\n","\n","# Session End\n","\n","@tool\n","def end_session(session_id: str) -> str:\n","    \"\"\"\n","    Delete the short-term (session) vector store to forget context.\n","    \"\"\"\n","    name = get_st_collection_name(session_id)\n","    try:\n","        chroma_client.delete_collection(name)\n","    except InvalidCollectionException:\n","        pass\n","    return f\"Short-term memory for session '{session_id}' cleared.\""],"metadata":{"id":"ZU3-RFdrk4OD","executionInfo":{"status":"ok","timestamp":1760735187337,"user_tz":-360,"elapsed":66,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# State + Agent Node\n","\n","class State(TypedDict):\n","    messages: Annotated[List[Any], add_messages]\n","    session_id: str                   # short-term session scope\n","    conversation_id: str              # long-term user/conv scope\n","    uploaded_files: List[str]         # paths for PDFs in this session\n","\n","# Tools registry\n","TOOLS = [\n","    ingest_pdfs,\n","    summarize_current_pdf,\n","    keyword_extract,\n","    save_insight,\n","    ask_with_memory,\n","    multiply,\n","    summarize_pdfs_to_longterm,\n","    cross_reference,\n","    memory_dashboard,\n","    memory_dashboard_markdown,\n","    end_session\n","]\n","\n","llm_with_tools = llm.bind_tools(TOOLS)\n","\n","def _blend_context_with_retrieval(state: State) -> str:\n","    \"\"\"\n","    Retrieve best-effort context for last user message from ST + LT (for prompting).\n","    Prefers including both, but final answers should still cite via tools.\n","    \"\"\"\n","    session_id = state.get(\"session_id\", \"\")\n","    last_user = \"\"\n","    for m in reversed(state[\"messages\"]):\n","        if isinstance(m, HumanMessage) or (isinstance(m, tuple) and m[0] == \"user\"):\n","            last_user = m.content if hasattr(m, \"content\") else m[1]\n","            break\n","    if not last_user:\n","        return \"\"\n","\n","    st_docs: List[str] = []\n","    if session_id:\n","        st_collection = get_or_create_st_collection(session_id)\n","        st_docs = query_collection(st_collection, last_user, n=4, where={\"session_id\": session_id})\n","\n","    lt_docs = query_collection(lt_collection, last_user, n=4)\n","\n","    st_blob = \"\\n\\n\".join([f\"[ST{i+1}] {d}\" for i, d in enumerate(st_docs)]) if st_docs else \"\"\n","    lt_blob = \"\\n\\n\".join([f\"[LT{i+1}] {d}\" for i, d in enumerate(lt_docs)]) if lt_docs else \"\"\n","\n","    context = \"\\n\\n\".join([b for b in [st_blob, lt_blob] if b])\n","    return context\n","\n","def agent_node(state: State):\n","    \"\"\"\n","    Main LLM node:\n","    - Retrieves context (ST + LT) to prime the LLM\n","    - Lets the LLM decide whether to call tools via tools_condition.\n","    \"\"\"\n","    context = _blend_context_with_retrieval(state)\n","    if context:\n","        messages_for_llm = [\n","            SystemMessage(content=\"You are a rigorous research assistant. Use the provided context when relevant.\"),\n","            HumanMessage(content=f\"Context:\\n{context}\")\n","        ] + state[\"messages\"]\n","    else:\n","        messages_for_llm = state[\"messages\"]\n","\n","    return {\"messages\": [llm_with_tools.invoke(messages_for_llm)]}"],"metadata":{"id":"FVgaK-VrlGVh","executionInfo":{"status":"ok","timestamp":1760735202080,"user_tz":-360,"elapsed":53,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Graph Assembly\n","memory = MemorySaver()\n","\n","builder = StateGraph(State)\n","builder.add_node(\"agent\", agent_node)\n","builder.add_node(\"tools\", ToolNode(TOOLS))\n","builder.add_edge(START, \"agent\")\n","builder.add_conditional_edges(\"agent\", tools_condition)\n","builder.add_edge(\"tools\", END)\n","\n","graph = builder.compile(checkpointer=memory)"],"metadata":{"id":"MBNb3DYOlL6Q","executionInfo":{"status":"ok","timestamp":1760735204943,"user_tz":-360,"elapsed":15,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#  Evaluation\n","# Session A: Ingest PDFs and ask questions (SHORT-TERM in action)\n","session_a = \"session_A_001\"\n","cfg_a = {\"configurable\": {\"thread_id\": session_a}}\n","\n","pdf_paths = [\"/content/SA_MLP__Accurate_and_Interpretable_Breast_Cancer_Detection_with_Feature_Selection_and_Explainable_AI.pdf\"]\n","r0 = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"Please ingest these PDFs: {pdf_paths}. Use session_id='{session_a}'.\")],\n","  \"session_id\": session_a,\n","  \"conversation_id\": \"user_abc\",\n","  \"uploaded_files\": pdf_paths\n","}, cfg_a)\n","print(\"\\n[Agent] Ingestion response:\\n\", r0[\"messages\"][-1].content)\n","\n","\n","# Ask a session-specific question\n","q1 = \"What are the main contributions listed in the paper?\"\n","r1 = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"Question about the PDF (session {session_a}): {q1}\")],\n","  \"session_id\": session_a,\n","  \"conversation_id\": \"user_abc\",\n","  \"uploaded_files\": pdf_paths\n","}, cfg_a)\n","print(\"\\n[Agent] Q1 answer:\\n\", r1[\"messages\"][-1].content)\n","\n","\n","# Save an insight to LONG-TERM memory\n","insight_text = \"The paper introduces the SA based Algorithm and Grid search framework.\"\n","r2 = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"Save this as a long-term insight: {insight_text}\")],\n","  \"session_id\": session_a,\n","  \"conversation_id\": \"user_abc\"\n","}, cfg_a)\n","print(\"\\n[Agent] Save insight response:\\n\", r2[\"messages\"][-1].content)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nOoWtyVl1U4","executionInfo":{"status":"ok","timestamp":1760735251664,"user_tz":-360,"elapsed":15308,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}},"outputId":"fa5de41a-d7ba-4672-ce33-fcb859352906"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:00<00:00, 104MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","[Agent] Ingestion response:\n"," {\"indexed_files\": [{\"file\": \"SA_MLP__Accurate_and_Interpretable_Breast_Cancer_Detection_with_Feature_Selection_and_Explainable_AI.pdf\", \"chunks\": 31, \"doc_id\": \"session_A_001::SA_MLP__Accurate_and_Interpretable_Breast_Cancer_Detection_with_Feature_Selection_and_Explainable_AI.pdf::df6af01b\"}]}\n","\n","[Agent] Q1 answer:\n"," **Main Contributions**\n","\n","1. **Optimized learning pipeline** – The study introduces a two‑stage optimization: simulated‑annealing–based feature selection followed by grid‑search hyperparameter tuning. This pipeline delivers a robust model (SA‑MLP) that achieves the highest reported accuracy on the Wisconsin Breast Cancer Dataset (WBCD) at 98.60 % [S1].\n","\n","2. **Systematic data preparation and model tuning** – A comprehensive methodology is presented that includes careful data cleaning, feature selection, and exhaustive hyperparameter tuning. This process not only maximizes predictive performance but also enhances interpretability for clinical decision‑making [S3].\n","\n","3. **Robustness to class imbalance** – Evaluation metrics (precision, recall, F1, weighted averages) demonstrate that the proposed models (MLP, XGBoost, SVC, Random Forest) effectively handle imbalanced classes, achieving near‑perfect recall and high F1 scores (e.g., XGBoost: 0.9940 precision, 1.0000 recall; MLP: 0.9987 balanced score) [S4].\n","\n","4. **Integration of advanced deep‑learning techniques** – The work incorporates a Dynamic Harris Hawks Optimized Gated Recurrent Unit (DDU) architecture, achieving 98.05 % accuracy, 98.09 % precision, and 98.28 % F1 on the same dataset. It also benchmarks against a wide range of conventional ML algorithms (logistic regression, decision trees, RF, SVM, NB, K‑NN, XGBoost), highlighting the competitive edge of deep‑learning approaches [S6].\n","\n","5. **Future clinical applicability and dataset expansion** – The authors outline plans to deploy the model in real‑world clinical settings, expand the training corpus with larger and more diverse datasets, and explore more sophisticated deep‑learning methods to further improve classification accuracy and robustness [S1, S5].\n","\n","These contributions collectively advance breast‑cancer detection by delivering a highly accurate, interpretable, and clinically viable machine‑learning framework.\n","\n","[Agent] Save insight response:\n"," Saved long-term insight: ins_249b7d7e72\n"]}]},{"cell_type":"code","source":["# Summarize all PDFs in the session to LONG-TERM memory (BONUS 1)\n","r2b = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"summarize_pdfs_to_longterm(session_id='{session_a}')\")],\n","  \"session_id\": session_a,\n","  \"conversation_id\": \"user_abc\"\n","}, cfg_a)\n","print(\"\\n[Agent] PDF summaries -> LT:\\n\", r2b[\"messages\"][-1].content)\n","\n","# Cross-reference a query across multiple PDFs + LT (BONUS 2)\n","r2c = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"cross_reference(query='How do they handle positional encoding?', session_id='{session_a}', k_per_file=3, k_lt=3)\")],\n","  \"session_id\": session_a,\n","  \"conversation_id\": \"user_abc\"\n","}, cfg_a)\n","print(\"\\n[Agent] Cross-reference answer:\\n\", r2c[\"messages\"][-1].content)\n","\n","#Visualize memory usage and retrieval trace (BONUS 3)\n","r2d = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"memory_dashboard_markdown(session_id='{session_a}', last_query='Transformer scalability')\")],\n","  \"session_id\": session_a,\n","  \"conversation_id\": \"user_abc\"\n","}, cfg_a)\n","print(\"\\n[Agent] Memory Dashboard (Markdown):\\n\", r2d[\"messages\"][-1].content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kd4D6X7TsyU-","executionInfo":{"status":"ok","timestamp":1760735302020,"user_tz":-360,"elapsed":45870,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}},"outputId":"4d18dc88-3ef3-469e-8421-d5a4e9ce3701"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[Agent] PDF summaries -> LT:\n"," No session PDFs found to summarize.\n","\n","[Agent] Cross-reference answer:\n"," No relevant evidence found in session PDFs or long-term memory.\n","\n","[Agent] Memory Dashboard (Markdown):\n"," Error: AttributeError(\"'str' object has no attribute 'parent_run_id'\")\n"," Please fix your mistakes.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2019503163.py:169: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  data = memory_dashboard(session_id, last_query)\n"]}]},{"cell_type":"code","source":["#Session B: New session, reuse LONG-TERM memory\n","session_b = \"session_B_009\"\n","cfg_b = {\"configurable\": {\"thread_id\": session_b}}\n","\n","q2 = \"Remind me what that paper contributed overall.\"\n","r3 = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"In a new session ({session_b}), {q2}\")],\n","  \"session_id\": session_b,\n","  \"conversation_id\": \"user_abc\"\n","}, cfg_b)\n","print(\"\\n[Agent] Q2 (new session, uses LT):\\n\", r3[\"messages\"][-1].content)\n","\n","#Explicit blended query tool (ST + LT)\n","r4 = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"ask_with_memory(query='{q1}', session_id='{session_a}', k_st=5, k_lt=5)\")],\n","  \"session_id\": session_a,\n","  \"conversation_id\": \"user_abc\"\n","}, cfg_a)\n","print(\"\\n[Agent] ask_with_memory result:\\n\", r4[\"messages\"][-1].content)\n","\n","#End Session\n","r_end = graph.invoke({\n","  \"messages\": [HumanMessage(content=f\"end_session(session_id='{session_a}')\")],\n","  \"session_id\": session_a,\n","  \"conversation_id\": \"user_abc\"\n","}, cfg_a)\n","print(\"\\n[Agent] End session A:\\n\", r_end[\"messages\"][-1].content)\n"],"metadata":{"id":"d-1c9VqY5XTF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760735365196,"user_tz":-360,"elapsed":63172,"user":{"displayName":"Shamik Dey","userId":"08157346527646763341"}},"outputId":"f2110d4f-cad9-48ea-c8d1-1191d9504bfb"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[Agent] Q2 (new session, uses LT):\n"," I’m happy to help, but I need to know which paper you’re referring to. Could you let me know the title, authors, or any key details so I can pull up the right information?\n","\n","[Agent] ask_with_memory result:\n"," **Main contributions of the paper**\n","\n","1. **Proposed a novel SA‑MLP model** for breast‑cancer diagnosis that integrates *Simulated Annealing* (SA) for feature selection with a standard Multi‑Layer Perceptron (MLP) classifier.  \n","   *SA* reduces the dimensionality of the Wisconsin Breast Cancer Dataset (WBCD) while preserving discriminative information, and the resulting SA‑MLP achieves an accuracy of **98.60 %**【ST5】.  \n","\n","2. **Comprehensive hyper‑parameter optimisation** using Grid Search.  The best‑performing settings for each algorithm (MLP, XGBoost, SVC, Random Forest) are reported and used to evaluate the models under 5‑fold cross‑validation【ST1】【ST4】.  \n","\n","3. **Extensive evaluation and comparison** with state‑of‑the‑art methods.  The paper presents precision, recall, F1‑score, confusion matrices, and AUC‑ROC curves for all models and shows that SA‑MLP outperforms previous approaches such as Gradient Boosting, ELRL‑E, OSELM, Stacking Ensemble + CNN, and DHH‑GRU (all on WBCD)【ST5】【ST2】.  \n","\n","4. **Demonstrated robustness to class imbalance**.  The SA‑MLP achieves perfect recall (1.0000) while maintaining high precision (0.9940), illustrating its ability to correctly identify minority‑class (malignant) cases【ST2】.  \n","\n","5. **Visualization of model performance**.  Confusion matrices and ROC curves (Fig. 4) are provided to illustrate how the models discriminate between benign and malignant classes and to pinpoint error patterns【ST1】【ST4】.  \n","\n","6. **Future‑work roadmap**.  The authors outline plans to deploy the SA‑MLP in clinical settings, enlarge the training dataset, and experiment with more sophisticated deep‑learning techniques to further improve performance and robustness【ST5】.  \n","\n","These contributions collectively present a new, high‑accuracy breast‑cancer classification framework that improves upon existing methods both in terms of accuracy and generalization capability.\n","\n","[Agent] End session A:\n"," Short-term memory for session 'session_A_001' cleared.\n"]}]}]}